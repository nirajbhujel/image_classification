{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "8167b787-30bd-4479-bc97-e536126b20a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import json\n",
    "import yaml\n",
    "import pickle\n",
    "import math\n",
    "import shutil\n",
    "import hydra\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from collections import defaultdict, Counter, deque\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets.dataset import LaserImageDataset, get_transforms\n",
    "from datasets.preprocess import load_labels\n",
    "from visualization.vis_utils import generate_html\n",
    "from models.network import Network\n",
    "\n",
    "from utils.metrics import MetricLogger\n",
    "from utils.misc import create_new_dir, copy_src, save_file\n",
    "from utils.utils import set_random_seed, model_parameters\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45892801-01f6-43f0-bc94-51cc64cdca61",
   "metadata": {},
   "source": [
    "## Specify the experiment to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "9672e111-2a51-4857-adf1-2cfdb8cad2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating exp : exp5_PointCNN_lr0.0003_e500_b32/Grating_A6\n"
     ]
    }
   ],
   "source": [
    "session = '002'\n",
    "# exp_dir = 'exp1_PointCNN_Grating_A6_Grating_SON1_LASEROPTIK_LASEROPTIK_SON1_seed3061994_lr0.0003_e500_b64'\n",
    "# exp_dir = 'exp2_THALES_SESO_10000_THALES_SESO_A_MLP_seed3061994_lr0.0003_e500_b32'\n",
    "# exp_dir = 'exp3_THALES_SESO_10000_THALES_SESO_A_PointCNN_lr0.0003_e500_b32'\n",
    "# exp_dir = 'exp3_LASEROPTIK_SON1_LASEROPTIK_LASEROPTIK_10000_PointCNN_lr0.0003_e500_b32'\n",
    "# exp_dir = 'exp3_Grating_A6_Grating_SON1_PointCNN_lr0.0003_e500_b32'\n",
    "\n",
    "# exp_dir = 'exp4_Grating_A6_PointCNN_lr0.0003_e500_b32'\n",
    "# exp_dir = 'exp4_Grating_SON1_PointCNN_lr0.0003_e500_b32'\n",
    "# exp_dir = 'exp4_LASEROPTIK_PointCNN_lr0.0003_e500_b32'\n",
    "# exp_dir = 'exp4_LASEROPTIK_SON1_PointCNN_lr0.0003_e500_b32'\n",
    "# exp_dir = 'exp4_LASEROPTIK_10000_PointCNN_lr0.0003_e500_b32'\n",
    "# exp_dir = 'exp4_THALES_SESO_A_PointCNN_lr0.0003_e500_b32'\n",
    "# exp_dir = 'exp4_THALES_SESO_10000_PointCNN_lr0.0003_e500_b32'\n",
    "\n",
    "exp_dir = 'exp6_augphoto0.5_PointCNN_lr0.0003_e500_b32'\n",
    "\n",
    "test_dataset = 'Grating_A6'\n",
    "# test_dataset = 'Grating_SON1'\n",
    "# test_dataset = 'LASEROPTIK_SON1'\n",
    "# test_dataset = 'THALES_SESO_10000'\n",
    "# test_dataset = 'THALES_SESO_A'\n",
    "# test_dataset = 'LASEROPTIK'\n",
    "# test_dataset = 'LASEROPTIK_10000'\n",
    "\n",
    "exp_dir = 'exp5_PointCNN_lr0.0003_e500_b32' + '/' + test_dataset\n",
    "\n",
    "print(\"Evaluating exp :\", exp_dir)\n",
    "\n",
    "log_dir = f\"../logs/session{session}/{exp_dir}\"\n",
    "ckpt_dir = log_dir + \"/checkpoints\"\n",
    "\n",
    "if not os.path.exists(log_dir):\n",
    "    raise Exception(f\"{log_dir} doesn't exists\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef51de7-a109-4f03-b9f2-8526f572683f",
   "metadata": {},
   "source": [
    "## Load cfg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "25a572a8-7e2d-454b-9857-43fd26ef106d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config\n",
    "cfg = OmegaConf.load(log_dir + '/cfg.yaml')\n",
    "# print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6a405f-928e-4db6-94e7-bb24f7583d86",
   "metadata": {},
   "source": [
    "## Load model from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "4b87d8b6-7990-4832-a71e-1d5bdf67d42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Checkpoint from  ../logs/session002/exp5_PointCNN_lr0.0003_e500_b32/Grating_A6/checkpoints/best_acc\n",
      "Network(\n",
      "  (net): PointCNN(\n",
      "    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (res1_conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (res1_conv2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (res1_conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (res2_conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (res2_conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (res2_conv3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (res2_skip): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (head): ClassificationHead(\n",
      "    (flat): Flatten(start_dim=1, end_dim=-1)\n",
      "    (act): ReLU()\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (fc1): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (fc3): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Model loaded sucessfully!\n"
     ]
    }
   ],
   "source": [
    "ckpt_name = 'best_acc'\n",
    "print(\"Loading Checkpoint from \", ckpt_dir + '/' + ckpt_name)\n",
    "network = Network(cfg)\n",
    "print(network)\n",
    "\n",
    "states = torch.load(ckpt_dir + '/' + ckpt_name + '.pth')\n",
    "if ckpt_name == 'model_states':\n",
    "    states = states['model_state']\n",
    "\n",
    "network.load_state_dict(states, strict=True)\n",
    "network = network.eval()\n",
    "print(\"Model loaded sucessfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce655bdf-c977-4e18-ad83-c6ee73484be0",
   "metadata": {},
   "source": [
    "## Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "9feb4b5f-8372-4fab-a1f7-bea243a064d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Grating_SON1']\n",
      "Class 0: 10267/17002 test samples\n",
      "Class 1: 6735/17002 test samples\n",
      "Evaluation samples: 17002, Bathces: 266\n"
     ]
    }
   ],
   "source": [
    "# eval_datasets = ['Grating_A6']\n",
    "# eval_datasets = ['THALES_SESO_A']\n",
    "# eval_datasets = ['THALES_SESO_A', 'THALES_SESO_10000']\n",
    "print(cfg.data.test_datasets)\n",
    "\n",
    "cfg.data.prefetch = False\n",
    "\n",
    "eval_dataset = LaserImageDataset(cfg,\n",
    "                                 phase='test',\n",
    "                                 img_labels=load_labels(cfg.data.data_dir, cfg.data.test_datasets),\n",
    "                                 img_transform=get_transforms(cfg.data, \"eval\"),\n",
    "                                )\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=64, shuffle=False, num_workers=1)\n",
    "\n",
    "print(f\"Evaluation samples: {len(eval_dataset)}, Bathces: {len(eval_dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5b184b-ccc5-457b-b055-10ec0570713e",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "178ce34d-c878-4a77-b7e6-3aa3d75fec59",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seed(cfg.exp.seed)\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "network = network.to(device)\n",
    "\n",
    "date_time = datetime.now().strftime(\"%d%m%Y\")\n",
    "# date_time = datetime.now().strftime(\"%d%m%Y_%H%M%S\")\n",
    "\n",
    "# result_dir = log_dir + f\"/evaluations/{date_time}\"\n",
    "result_dir = log_dir + \"/evaluations\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10569c06-7e3b-4e17-9de4-c83fd7cfb38a",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1803093f-57c8-4ec6-97f7-cb3474a60037",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [04:16<00:00,  1.04it/s, 1.000000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating exp5_PointCNN_lr0.0003_e500_b32/Grating_A6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 11%|█▏        | 30/266 [00:10<01:22,  2.85it/s, 1.000000]"
     ]
    }
   ],
   "source": [
    "from utils.metrics import MetricLogger\n",
    "# from torchmetrics.classification import BinaryAccuracy\n",
    "# eval_metrics = BinaryAccuracy().to(device)\n",
    "\n",
    "eval_metrics = MetricLogger()\n",
    "\n",
    "eval_results = []\n",
    "pbar = tqdm(total=len(eval_dataloader), position=0)\n",
    "print('Evaluating', exp_dir)\n",
    "with torch.no_grad():\n",
    "    for iter, batch in enumerate(eval_dataloader):\n",
    "        pbar.update(1)\n",
    "\n",
    "        imgs, labels, img_files = batch\n",
    "\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        preds = network(imgs)\n",
    "        preds = F.softmax(preds, dim=-1)\n",
    "        \n",
    "        acc = eval_metrics(preds.argmax(-1).to(device), labels.argmax(-1).to(device))\n",
    "        # acc = eval_metrics.update(preds.to(device), labels.to(device))\n",
    "\n",
    "        for f, l, p in zip(img_files, labels, preds):\n",
    "            eval_results.append(dict(img=f, label=l.argmax().item(), pred=p.argmax().item(), prob=p.max().item()))\n",
    "            \n",
    "        pbar.set_postfix_str(f\"{acc:03f}\")\n",
    "        # break\n",
    "\n",
    "eval_metrics.compute()\n",
    "\n",
    "try:\n",
    "    cm = confusion_matrix([item['label'] for item in eval_results], [item['pred'] for item in eval_results])\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=[0,1]).plot()\n",
    "    disp.figure_.savefig(log_dir + '/eval_confusion_matrix.png')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "with open(log_dir + '/eval_metric_results.txt', 'w') as f:\n",
    "    json.dump(eval_metrics.results, f)\n",
    "    \n",
    "with open(log_dir+'/eval_results.pkl', 'wb') as f:\n",
    "    pickle.dump(eval_results, f)\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), \"../data/near_field/images/\")\n",
    "html_data = generate_html(eval_results, data_dir, title=\"Eval Results\", save_dir=log_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c818be-fbc0-4f3e-94de-31ad18f2e761",
   "metadata": {},
   "source": [
    "##  Remove some false positives and test on real dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab55dcf-9a84-41b6-a716-c590d9acedb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load eval results\n",
    "# exp_dir = 'exp3_Grating_A6_Grating_SON1_PointCNN_lr0.0003_e500_b32'\n",
    "# exp_dir = 'exp4_Grating_A6_PointCNN_lr0.0003_e500_b32'\n",
    "exp_dir = 'exp4_Grating_SON1_PointCNN_lr0.0003_e500_b32'\n",
    "# exp_dir = 'exp4_LASEROPTIK_PointCNN_lr0.0003_e500_b32'\n",
    "# exp_dir = 'exp4_LASEROPTIK_SON1_PointCNN_lr0.0003_e500_b32'\n",
    "# exp_dir = 'exp4_LASEROPTIK_10000_PointCNN_lr0.0003_e500_b32'\n",
    "# exp_dir = 'exp4_THALES_SESO_A_PointCNN_lr0.0003_e500_b32'\n",
    "# exp_dir = 'exp4_THALES_SESO_10000_PointCNN_lr0.0003_e500_b32'\n",
    "\n",
    "eval_dir = f\"../logs/session002/{exp_dir}\"\n",
    "with open(eval_dir + '/eval_results.pkl', 'rb') as f:\n",
    "    eval_results = pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e096fb8c-0c23-476f-9274-dca5b5233bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by dataset and location\n",
    "dset_results = defaultdict(lambda: defaultdict(list))\n",
    "for item in sorted(eval_results, key=lambda x: x['img']):\n",
    "    dset, loc, filename = item['img'].split('/')\n",
    "    dset_results[dset][loc].append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f658f456-69ce-465b-8580-0a93e57a63ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_preds(y_true, y_preds, title='', figsize=(8,5), fontsize=10):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=figsize)\n",
    "    ax.plot(y_true, color='b', label='GT')\n",
    "    ax.plot(y_preds, color='r', label='Pred')\n",
    "    \n",
    "    # Set ticks and labels\n",
    "    ax.set_yticks([0, 0.5, 1], ['Undamaged', '0.5', 'Damaged'], fontsize=fontsize)\n",
    "    ax.set_ylim(-0.1, 1.1)  # Extend y-axis slightly for better visibility\n",
    "    ax.set_xlabel('Frames', fontsize=fontsize)\n",
    "\n",
    "    # Set title\n",
    "    ax.set_title(title, fontsize=fontsize)\n",
    "\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend(fontsize=fontsize)\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "# Remove some false positives\n",
    "conf_th = 0.5\n",
    "fp_count_th = 10\n",
    "moving_avg_window = 5\n",
    "moving_avg_th = 0.5\n",
    "all_acc = []\n",
    "all_lag = []\n",
    "plt.close('all')\n",
    "for dset, dset_preds in dset_results.items():\n",
    "    for loc, loc_preds in dset_preds.items():\n",
    "        \n",
    "        # if dset!='Grating_SON1' :\n",
    "        #     continue\n",
    "        # if loc not in ['01_02', '05_01', '05_03']:\n",
    "        # # if loc not in ['04_01']:\n",
    "        #     continue\n",
    "            \n",
    "        # Initialize a deque to store the last 'window_size' predictions\n",
    "        pred_window = deque([], maxlen=moving_avg_window)\n",
    "        gt_labels, pred_labels = [], []\n",
    "        \n",
    "        damage_start_index = None\n",
    "        detection_index = None\n",
    "    \n",
    "        fp_count = 0\n",
    "        lag = 0\n",
    "        for i, item in enumerate(loc_preds):\n",
    "            gt_label= item['label']\n",
    "            \n",
    "            # Add the current prediction to the window\n",
    "            pred_window.append(item['pred'])\n",
    "            \n",
    "            # Calculate moving average\n",
    "            moving_avg = sum(pred_window) / len(pred_window)\n",
    "            # Filter prediction based on moving average\n",
    "\n",
    "            if (item['pred']==1) & (item['prob']>0.9):\n",
    "                fp_count += 1\n",
    "            else:\n",
    "                fp_count = 0\n",
    "\n",
    "            if fp_count>fp_count_th:\n",
    "                pred_label = 1\n",
    "            elif (moving_avg > moving_avg_th) & (item['prob']>conf_th):\n",
    "                pred_label = 1\n",
    "            else:\n",
    "                pred_label = 0\n",
    "            \n",
    "            pred_labels.append(pred_label)\n",
    "            gt_labels.append(item['label'])\n",
    "            \n",
    "            # Lag measurement logic\n",
    "            if gt_label == 1 and damage_start_index is None:\n",
    "                damage_start_index = i\n",
    "            \n",
    "            if (pred_label == 1) and (detection_index is None) and (damage_start_index is not None):\n",
    "                detection_index = i\n",
    "                lag = detection_index - damage_start_index\n",
    "                # mean_delay.append(lag)\n",
    "                # print(f\"Damage detected with a lag of {lag} time steps\")\n",
    "                # damage_start_index = None\n",
    "                # detection_index = None\n",
    "            \n",
    "        acc = np.equal(gt_labels, pred_labels).mean() * 100\n",
    "        all_acc.append(acc)\n",
    "        all_lag.append(lag)\n",
    "        \n",
    "        title = f\"{dset}/{loc}, acc:{acc:.2f}%, lag:{lag/10:.2f}secs\"\n",
    "        fig, ax = plot_preds(gt_labels, pred_labels, title=title)\n",
    "        fig.savefig(f\"{log_dir}/{dset}_{loc}.png\")\n",
    "        \n",
    "print(f\"Mean ACC: {np.mean(all_acc):.2f}, Mean Delay: {np.mean(all_lag):.2f}secs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "laser-damage",
   "language": "python",
   "name": "laser-damage"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
